

KubeMin-Cli 是想做一个对开发人员更加友好的平台，以轻量化的工作流的方式来构建用户的应用，KubeMin-Cli项目中的App结构模型基于OAM 而Kubevela的问题在于只适合运维人员，因为它在呈现出来的配置化，描述的那些特征都是基于k8s的底层资源，这对开发者来说需要一定的认知，无形中也是提高了门槛和复杂性。KubeMin-Cli 希望降低使用的门槛，降低这些复杂性，构建更加通用的数据结构。

## 架构图



## 模型构建



### 特征

Traits属性用于为"组件"附加一些特性，比如为组件附加新增存储支持，新增"自定义边车容器"等特性

```Json
"traits": {
  "storage": [
          {
            "type":"persistent", //稳定存储
            "mountPath":"/data",
            "size": "20",
          },
          {
            "type":"ephemeral" //暂时存储
            "mountPath":"/data",
          },
          {
            "type":"config" //配置文件信息(限制大小)
            "name": "config-1",//
          },
          {
            "type":"secret" //密钥
            "name": "secret-1",
          },
        ],
      }
```



#### storage 存储

Storage支持多种存储方式，比如`persistent`稳定存储，它对应的是Kubernetes中的 PersistentVolumeClaim 和 VolumeMount 属性。在KubeMin-Cli中为了简化表达使用: `{"type":"persistent","mountPath":"/data","size": "20"}`Json格式对组件提出存储方面的附加追述。

| **type**     | **对应 Kubernetes 资源**            | 说明                                                         |
| ------------ | ----------------------------------- | ------------------------------------------------------------ |
| persistent   | PersistentVolumeClaim + VolumeMount | 稳定的存储，使用`PersistentVolumeClaim`卷用于将持久卷(PersistentVolume)挂载到Pod中。这种方式可以为Pod提供一个稳定的存储方式，不会因为重启/Pod崩溃而丢失容器内持久化存储的文件。 |
| ephemeral    | emptyDir + VolumeMount              | 临时存储，对于定义了 `emptyDir` 卷的 Pod，在 Pod 被指派到某节点时此卷会被创建。 就像其名称所表示的那样，`emptyDir` 卷最初是空的。尽管 Pod 中的容器挂载 `emptyDir` 卷的路径可能相同也可能不同，但这些容器都可以读写 `emptyDir` 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，`emptyDir` 卷中的数据也会被永久删除。 |
| host-mounted | hostPath + VolumeMount              | hostPath 可以让主机节点文件系统上的文件或目录挂载到Pod中，比如运行一个需要访问节点级系统组件的容器；让存储在主机系统上的配置文件可以被[静态 Pod](https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/static-pod/) 以只读方式访问 |
| config       | ConfigMap + VolumeMount 或 EnvFrom  | 如果在Storage特征中定义config那么KubeMin会将其视为:在Pod中将ConfigMap当作文件使用 |
| secret       | Secret + VolumeMount 或 EnvFrom     | 如果在Storage特征中定义secret那么KubeMin会将其视为:在Pod中将secret当作文件使用 |



##### 例子

在Storage存储中创建的Config一定得是ConfigMap:{"defaultMode": "420","name": "m2507151323j3fnrk-mysql"}

```json
//如果创建了一个稳定存储，那么意味着一定会创建一个名为data的PVC
{
  "name":"data",
  "type":"persistent", //稳定存储
  "mountPath":"/data",
  "size": "20Gi",
},
//如果创建了一个临时存储，那么意味着volumes里面会多一个名为conf的emptyDir类型的配置
{
	"name":"conf",
	"type":"ephemeral" //临时存储
}
//如果创建了一个Config配置类型，那么意味着volumes里面会多一个 -configMap: defaultMode: 420 name: m2507151323j3fnrk-mysql的配置
{
  "name": "config-map",
  "type": "config", //配置文件挂载
  "sourceName":"m2507151323j3fnrk-mysql",
}
```



#### Env And EnvFrom 环境变量

在Kubernetes中使用环境变量分为几种:

1.直接在Container设置Env属性，将键值对写成环境变量

2.使用ConfigMap或Secret将环境配置信息和容器镜像解耦，以便于应用配置的修改。在这种情况下又分为几种细项：

2.1 引用ConfigMap或Secret中的信息，但是重新命名环境变量名

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-demo-pod
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      env:
        # 定义环境变量
        - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的
          valueFrom:
            configMapKeyRef:
              name: game-demo           # 这个值来自 ConfigMap
              key: player_initial_lives # 需要取值的键
        - name: UI_PROPERTIES_FILE_NAME
          valueFrom:
            configMapKeyRef:
              name: game-demo
              key: ui_properties_file_name
      volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
  volumes:
  # 你可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中
  - name: config
    configMap:
      # 提供你想要挂载的 ConfigMap 的名字
      name: game-demo
      # 来自 ConfigMap 的一组键，将被创建为文件
      items:
      - key: "game.properties"
        path: "game.properties"
      - key: "user-interface.properties"
        path: "user-interface.properties"
```

这段配置相当于引用`game-demo`这个ConfigMap里面的`player_initial_lives`Key的值，但是在Pod中它的环境变量名被重新命名为：`PLAYER_INITIAL_LIVES`，不再是`player_initial_lives`

```yaml
 volumeMounts:
      - name: config
        mountPath: "/config"
        readOnly: true
volumes:
  # 你可以在 Pod 级别设置卷，然后将其挂载到 Pod 内的容器中
  - name: config
    configMap:
      # 提供你想要挂载的 ConfigMap 的名字
      name: game-demo
      # 来自 ConfigMap 的一组键，将被创建为文件
      items:
      - key: "game.properties"
        path: "game.properties"
      - key: "user-interface.properties"
        path: "user-interface.properties"
```

这一段的作用是将game-demo这个ConfigMap中的两个key挂载到/config目录下:

```
# ls
game.properties  user-interface.properties
# cat game.properties
enemy.types=aliens,monsters
player.maximum-lives=5
```

在KubeMin-Cli是如何实现上述场景的？使用特征中的Envs结构来描述：

```json
{
  "name": "my-awesome-app",
  "componentType": "webservice",
  "image": "nginx:latest",
  "replicas": 2,
  "traits": {
    "envs": [
      {
        "name": "DATABASE_PASSWORD",
        "valueFrom": {
          "secret": {
            "name": "project/db-credentials",
            "key": "password"
          }
        }
      },
      {
        "name": "API_ENDPOINT",
        "valueFrom": {
          "config": {
            "name": "environment/app-settings",
            "key": "api-url"
          }
        }
      }
    ]
  }
}
```

traits 中会使用 envs 结构来允许用户引用某个ConfigMap并自定义环境变量名称，目前仅实现ConfigMap和Secret，以及fieldRef。

> fieldRef 可以将Pod级别的字段作为环境变量投射到正在运行的容器中,例如 spec.nodeName metadata.namespace 等。
> 注意：这个字段变量指的是Pod字段，不是Pod中Container的字段。[详情可见](https://kubernetes.io/zh-cn/docs/tasks/inject-data-application/environment-variable-expose-pod-information/)


2.2 在Pod中将ConfigMap当作文件使用

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap
```

KubeMin-Cli 中将这种配置项当作文件使用的方式，使用特征中的**Storage**来描述它：

```json
"traits": {
  "storage": [
          {
            "name": "foo",//自定义挂载名,如果不定义则自动生成
            "sourceName":"myconfigmap",//引用ConfigMap资源的名称
            "type":"config",
            "mountPath":"/etc/foo",
            "readOnly":true //默认只读
          }
        ],
      }
```

关于被挂载的ConfigMap更新：当ConfigMap被更新时，在Pod中的键也会被更新。K8s中的kubelet组件会检查所挂载到Pod中的卷是否是最新，它会有一个机制来将ConfigMap中的信息同步到Pod中。但是它使用的是其本地的告诉缓存来获得ConfigMap的当前值。

> ConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL 的缓存，还可以直接经过所有请求重定向到 API 服务器。 因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去， 这一时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。 这里的传播延迟取决于所选的高速缓存类型 （分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。
>
> **以环境变量方式使用的 ConfigMap 数据不会被自动更新。 更新这些数据需要重新启动 Pod。**



2.3使用ConfigMap作为环境变量

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: env-configmap
spec:
  containers:
    - name: app
      command: ["/bin/sh", "-c", "printenv"]
      image: busybox:latest
      envFrom:
        - configMapRef:
            name: myconfigmap
```

在KubeMin-Cli中使用**EnvFrom**来描述这种将ConfigMap当作变量注入

```json
"traits": {
  "envFrom": [
          {
            "type":"config",
            "sourceName":"myconfigmap"
          }
        ],
      }
```

Pod中的环境变量名称允许的字符范围是有限的。如果某些变量名称不满足这些规则，即使Pod可以被启动，但是容器内的服务也是无法访问这些环境变量的。





### 特征的插件设计

单独为'特征'设计一个插件模块是为了为组件提供更丰富的扩展功能，比如在`StateFulSet`会需要InitContainers,自定义边车容器,命令行等。这里是想根据能附加的不同属性来统一管理。

```go
type TraitContext struct {
	Component *model.ApplicationComponent
	Workload  runtime.Object
	TraitData interface{}
}

type TraitResult struct {
	// Pod-level modifications
	InitContainers []corev1.Container
	Containers     []corev1.Container
	Volumes        []corev1.Volume
	VolumeMounts   map[string][]corev1.VolumeMount // Keyed by container name
	// Cluster-level objects to be created alongside the workload
  // 需要在集群中独立创建的新资源
	AdditionalObjects []client.Object
}

// TraitProcessor is the interface for all trait processors.
type TraitProcessor interface {
	Name() string
	Process(ctx *TraitContext) (*TraitResult, error)
}
```

TraitContext 始终贯穿整个对插件流，`Workload`字段代表的是**当前正在被处理的基础工作负载对象**，它有可能是一个 `Deployment` 或 `StatefulSet`指针对象的实例。它有两个核心作用:

1.提供只读的上下问信息(Providing Read-Only Context)，比如根据现有的配置来决定某个决策

示例: 想象一个名为 "auto-scaler" 的 Trait。它的 Process 方法可能需要读取 Workload 的 Spec.Replicas 字段，来决定 HorizontalPodAutoscaler (HPA) 的minReplicas 和 maxReplicas 应该设置成什么值。

通过 ctx.Workload，Trait 处理器可以访问到整个工作负载对象的完整信息，但我们的新设计不鼓励它直接修改这个对象（除了像 env 这样的少数“修改者”例外）。

2.作为“修改者”Trait 的直接操作目标 (Target for "Mutator" Traits)

虽然不鼓励直接对它进行修改，但是为了防止在某些特殊的复杂情况下实现特征太过于复杂，某些Trait的核心功能就是修改工作负载的本身，比如`Env`必须先获取Pod模版中的主容器，并向EnvFrom字段中添加某些内容。在这种情况下Workload字段充当了一个可变的对象引用，允许特定的、受控的副作用发生。



`TraitData`字段是**当前正在执行的那个Trait所需要的具体配置信息。**它的类型是`interface`这意味着它可以是任何类型，因为每一种Trait的配置结构都是不同的。比如 Storeage Trait 需要的类型是一个 []model.Storage 切片。

TraitData的作用就是充当一个通用的数据容器，当用户在 `ApplicationComponent.Traits`中配置了某些 Trait的配置，则原封不动传递给处理器。



`TraitResult`将整个插件的产出分为了两个大类：

1.Pod 内部的修改 (`InitContainers`, `Containers`, `Volumes`, `VolumeMounts`)定义所有需要被合并到主工作负载（如 Deployment 或 StatefulSet）的 PodTemplateSpec 中的资源。比如:

1.1.init Trait 会在 InitContainers 字段中返回一个 init 容器。

1.2.sidecar Trait 会在 Containers 字段中返回一个 sidecar 容器，并在 Volumes 和 VolumeMounts 中返回它可能需要的卷。

1.3.storage Trait 会在 Volumes 和 VolumeMounts 中返回它创建的卷和挂载点。

在 ApplyTraits 的最后阶段，applyTraitResultToWorkload 函数会遍历这些字段，并将它们 append 到 workload.Spec.Template.Spec 的相应位置。



2.集群级别的对象 (`AdditionalObjects`)

定义所有需要在集群中作为独立资源被创建的对象。当 storage Trait 为一个 Deployment 创建持久化存储时，它会在 AdditionalObjects 字段中返回一个 *corev1.PersistentVolumeClaim 对象。

如果后续实现中需要添加类似 Ingress Trait 可以在这里返回一个 *networkingv1.Ingress 对象，或一个 service-monitor Trait 可以在这里返回一个 *monitoringv1.ServiceMonitor 对象。

pplyTraits 函数会将所有 Trait 产生的 AdditionalObjects 收集起来，并作为最终的返回值 []client.Object 返回给调用者（例如 Run方法）。调用者需要遍历这个切片，并使用 client.Create() 将这些对象一一创建到集群中。

> 这里是为了以后做扩展，这里也可以定义不同的组件来创建不同的资源，如果强绑定的一些特征可以考虑单独在这里实现集群级别的对象。相当于两种实现方式来应对复杂环境中的模型定义。





### 策略

模型的策略包含：镜像拉取的策略，组件重启的策略，重试，探活等







## 工作流

工作流在执行时会将创建多个Job并行/串行执行，有一种场景是：

1.集群内相同命名空间内已部署一个服务。

1.1 如果仅仅只是镜像发生变化，则直接更改Deployment的镜像名

1.2 如果Deployment的元数据发生变化，是直接更改Deployment的信息比较好，还是删除后创建一个新的Deployment比较好？

> 一般是直接更改Deployment的信息，通过kubectl apply 或API的方式进行更新，Kubernetes 会自动对比并以最小的中断方式进行滚动更新。



2.如何对比两个Deployment的信息是否一致？

2.1两个Deployment以`client.AppsV1().Deployments`对象形式存在，这种场景应该如何对比？

> 对比Image 对比Pvc 对比Env 对比Sercet等信息(待定)



工作流的设计在于如何更好地执行某一类型的任务，有些任务不需要回调，有些任务则需要回调函数，有些任务是异步的，有些任务是需要及时返回的。应对这种类繁多的Job解决方案如下：

1.异步，不需要回调

任务请求通过现有的工作流执行，通过数据库的Job执行状态来判断是否最终执行成功。这类方式适合：不确定任务的执行时间的情况下使用，比如部署应用，执行脚本任务，某些一次性任务等。这些任务只需要记录是否执行成功，最终记录Job的执行状态。

2.异步，需要回调。

新增Job的类型即可，因为整个过程是异步的，可以通过上下文将整个任务全部记录下来，最终写到数据库，或消息中间件中。

2.1如果需要写到数据库中，那么就需要给予用户一个查询整个记录信息，可以通过API查询数据中所记录的日志信息

2.2如果写入消息中间件中，则可以做一些消息推送机制，比如另外一个服务用于接受kafka中的消息，然后通过ws主动推送到客户端。但是这种方式前期投入到成本要比第一种方式要高。它的好处在于易于扩展，可以基于不同的场景做更多的适配。

3.同步，不需要回调。

通过新增Job的类型即可实现，因为整个过程是同步的，用户可以基于这种方式执行一些轻量级任务，又因为用户可能不需要回调的信息，可以直接通过API风格的接口包装一层，直接使用封装好的函数执行一些通用命令。但是这里要注意默认框架的超时时间，在高并发的场景下可能会频繁超时。

为了避免高并发场景下的频繁执行，这里可能需要引入一些限流的机制，或任务池。

4.同步，需要回调用。

直接通过接口去实现，比如查询日志这种情况，可以封装成一个新的接口，然后实现不同的返回信息。客户端可以与服务端建立一个SSE，也可以建立一个WS服务，及时交换信息。

```
                   +-----------------------+
                   |        Job            |
                   +-----------------------+
                          |         |
                     +----+         +----+
                     |                    |
                Async Job             Sync Job
                 |    |                |     |
       +---------+    +----------+     |     +----------+
       |                       |       |                |
 No Callback           Need Callback  No CB        Need Callback
   |                         |         |               |
 DB记录状态       Kafka/WS 推送日志     API返回      SSE/WS 实时返回
```



Web界面的技术选型

前端使用React，工作流的部分使用ReactFlow来构建节点，Cli使用交互式对话来构建Json结构的KubeMin-Cli模型，这样可以实现Cli->生成工作流->任意导出/导入->web可视化。





## 分布式工作流

服务在初始启动时会通过`DetectReplicaCount`函数获取对应的副本数量，如果副本数量为偶数则让最后启动的Pod退出，以确保正常运行的服务数量为奇数。然后进入到领导人选举函数中`setupLeaderElection`该函数的主要作用是让同一服务多个副本中的其中一个为Leader，其他服务为Follower。leaderelection.RunOrDie(ctx, *cfg) 异步启动选举循环。



```go
func (s *restServer) setupLeaderElection(errChan chan error) (*leaderelection.LeaderElectionConfig, error) {
	restCfg := ctrl.GetConfigOrDie()
	ns := s.cfg.LeaderConfig.Namespace
	if ns == "" {
		ns = config.NAMESPACE
	}
	rl, err := resourcelock.NewFromKubeconfig(resourcelock.LeasesResourceLock, ns, s.cfg.LeaderConfig.LockName, resourcelock.ResourceLockConfig{
		Identity: s.cfg.LeaderConfig.ID,
	}, restCfg, time.Second*10)
	if err != nil {
		klog.ErrorS(err, "Unable to setup the resource lock")
		return nil, err
	}
	return &leaderelection.LeaderElectionConfig{
		Lock:          rl,
		LeaseDuration: time.Second * 15,
		RenewDeadline: time.Second * 10,
		RetryPeriod:   time.Second * 2,
		Callbacks: leaderelection.LeaderCallbacks{
            OnStartedLeading: func(ctx context.Context) {
                s.onStartedLeading(ctx, errChan)
            },
			OnStoppedLeading: func() {
				if s.cfg.ExitOnLostLeader {
					errChan <- fmt.Errorf("leader lost %s", s.cfg.LeaderConfig.ID)
				}
			},
			OnNewLeader: func(identity string) {
				if identity == s.cfg.LeaderConfig.ID {
					return
				}
				klog.Infof("new leader elected: %s", identity)
				// we are follower now; if distributed (>=3), ensure workers started
				cnt := kube.DetectReplicaCount(context.Background(), s.KubeClient)
				if cnt >= 3 {
					s.startWorkers(context.Background(), errChan)
				}
			},
		},
		ReleaseOnCancel: true,
	}, nil
}
```

1.使用`ctrl.GetConfigOrDie()`函数读取K8s集群配置

2.使用resourcelock.NewFromKubeconfig函数构建Lease锁，这是K8s Lease的机制，因为基于云原生，所以这里直接使用该机制实现选举。

下列配置暂时固定，后续可以通过配置文件来实现。

```
  - LeaseDuration: 15s，Leader 需要在该时间内续约，否则被认为失效。
  - RenewDeadline: 10s，Leader 续约的截止时间窗口。
  - RetryPeriod: 2s，争抢或续约的尝试间隔。
```

3.LeaderCallbacks结构体会设置三个函数，以在选举后实现不同的业务逻辑

```go
type LeaderCallbacks struct {
	// OnStartedLeading is called when a LeaderElector client starts leading
	OnStartedLeading func(context.Context)
	// OnStoppedLeading is called when a LeaderElector client stops leading.
	// This callback is always called when the LeaderElector exits, even if it did not start leading.
	// Users should not assume that OnStoppedLeading is only called after OnStartedLeading.
	// see: https://github.com/kubernetes/kubernetes/pull/127675#discussion_r1780059887
	OnStoppedLeading func()
	// OnNewLeader is called when the client observes a leader that is
	// not the previously observed leader. This includes the first observed
	// leader when the client starts.
	OnNewLeader func(identity string)
}
```

OnStartedLeading 开始担任领导者时，会触发这个事件。领导者服务需要执行四件事

```go
// onStartedLeading encapsulates responsibilities when this instance becomes leader.
// It starts leader-scoped services, ensures queue readiness, reconciles worker role,
// and spawns watchers for ongoing adjustments.
func (s *restServer) onStartedLeading(ctx context.Context, errChan chan error) {
    // Start event service (leader lifecycle)
    go event.StartEventWorker(ctx, errChan)

    // Ensure consumer group exists (best-effort) and start queue metrics
    s.ensureQueueGroup(ctx)
    s.startQueueMetrics(ctx)

    // Initial reconcile of worker role based on current replica count
    s.reconcileWorkers(ctx, errChan)

    // Periodically re-evaluate topology and reconcile role
    s.startReplicaWatcher(ctx, errChan)
}
```

3.1.1 使用协程执行Event中所有的Worker，这个Worker里面包含WorkflowTaskSender权限。如果是Noop模式下会使用轮询的模式，每三秒从数据库中获取数据加入到消息队列中;如果是分布式模式(v1)，则使用Redis的Streams作为消息队列。

3.1.2 ensureQueueGroup 函数幂等创建消费组

3.1.3 startQueueMetrics 是定时打印 backlog/pending的队列数量

3.1.4 reconcileWorkers 按照副本数量来启动或停止 workers，当副本数量大于等于3时领导者停止消费者服务的工作，当副本为1时则表明是单服务，需要既是消费者又是生产者。

3.1.5 startReplicaWatcher 每三十秒复查副本数量并调用 reconcileWorkers函数，来防止出现脑裂。



4.OnStoppedLeading 失去领导者时触发，若 ExitOnLostLeader=true，通过 errChan 上报错误，促使上层优雅退出（便于调试或避免脑裂）

5.OnNewLeader 当出现新的Leader且不是自己时，记录日志。若处于分布式（副本数 >= 3），Follower 侧确保启动 workers 参与消费。


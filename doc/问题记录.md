1.如果用户要描述的不只是"部署一个服务"，而是希望创建多个组件(Web+Worker+Redis);每个组件具有依赖关系(先部署Redis,再部署Web);有前置初始化任务(如建表);最后需要暴露一个统一的入口(如Api Gateway);那么平台应该如何构建？

> KubeMin-Cli 会借鉴OAM模型中的"组件"思想，通过组件化的拆分，将应用由一个或者多个组件构成。这里的每个组件在部署/更新时都会转换为不同类型的Job，通过模型的策略来决定系统的工作流引擎执行时是串行还是并行。
> 用户可以通过一系列编辑，最终决定组件的依赖关系，部署的先后顺序，执行哪些初始化任务或者需不需要暴露统一入口(Ingress或Istio)



2.如何将"组件之间的依赖关系+并行串行调度控制"映射为一套可执行的调度逻辑(轻量型Workflow Engine)

> 可以基于OAM模型添加一个 Workflow 的描述段：
> Workflow:
>
> Steps:
>
> ​    -name : deploy
>
> ​     Type: deploy
>
> ​     properties:
>
> ​       policies: ["first-app-front"]
>
> 这样只要系统实现其中的Type，那么意味着工作流可以自由编排，并将这部分的业务构建放入业务层。



3.如何让其他开发团队/业务组定义自己的任务类型？是否需要提供Job类型注册机制？如何避免"所有类型的区分变为If-else?"

> 其他的开发团队或业务组可以通过Rest 风格的API 来构建工作流，但是暂时不会提供JobPlugin接口的标准，当基础的Job Type比较完善后，会考虑开放这部分标准，让更多的开发者拥有自定义的能力。



4.如果某些业务团队希望定义一个Job，比如type:ai-eval 那么如何允许他们以“最小的侵入”注册他们的处理逻辑，并且在工作流引擎中自然生效？

> 如果业务团队希望定义一个Job 我应该会先实现一个新的Job类型，比如ai-eval 来实现GPU资源挂载到Pod内，或者绑定某些云平台的Key。在多租户的场景下，我会使用多集群+命名空间的方式来对Job进行物理和虚拟化的隔离。Job执行状态我会在ack函数中写入到数据库中，以供页面展示。



5.在 AI/GPU 场景中，资源紧张非常常见。那么你平台如何：预检目标集群是否有足够 GPU？任务是否可能长时间 Pending？用户是否能看到 Job 是“失败”还是“被平台拒绝调度”？

> AI/GPU 的场景中，我可以使用GPU的异构中间件来完成GPU资源分配，任务可以通过设置超时时间来选择Pending还是结束。无论是Cli 还是Web 都会跟踪所有Job的状态变化。如果因为被平台拒绝也会提示相应的状态或展示日志。



6.当多个业务团队注册了不同类型的 Job（如AI训练、Helm 安装、Chaos 工程、远程 SSH 执行），你如何：管理 Job 类型的一致性（文档/验证/注册）？避免单个 Job 实现引起平台级故障？控制 Job 对外资源的访问（如挂云盘、创建 Service）？

> 当多个业务团队注册了不同类型的 Job，我可以使用分布式锁，来管理Job的一致性。整个平台会基于K8s构建，那么这意味着所有的Job都会用命名空间和Pod来执行Job，我可以设定一些故障策略来清理，记录失败的Job，而这些Job不会引起平台级别的故障。
>
> k8s中可以使用网络策略来实现Job对外资源的访问，当集群的规模足够庞大时，我可以通过使用服务网格来控制流量，可以使用云Nas来实现底层的持久化。



7.当 Job 失败率上升、队列堆积、任务延迟时：你是否可以预警平台异常（非 Job 异常）？Job 是否有统一的指标（完成时间、排队时间、成功率）？你如何暴露这些指标给监控系统（Prometheus / Grafana）？

> 可以直接在集群中部署Prometheus，然后通过Loki等日志采集等服务，对整个集群进行监控。
> Job 会设置默认的超时时间，未来会提供一系列可视化看板，比如队列中当前存在哪些任务，已完成的任务时长，成功率等。
> Job所有的执行都会通过工作流引擎全程跟踪，它们的错误日志或状态都会持久化。



8.Kubernetes的核心思想是声明式的，用户应该关心“我需要什么”，而不是应该运行一个Job来描述我想要的东西是什么，在KubeMin-Cli中应该做的更加友好一点，可以使用一种混合模式来实现：

> 部分资源可以通过描述来直接创建，比如PVC，用户需要多大的存储空间，至于存在哪里(使用哪个SC，是否存在硬隔离，超限应该如何处理)这些可以通过默认的策略来适应不同的场景。
>
> 但是有一些是无法获取更多的信息的，比如在描述一个Statefulset的服务时，更多的是各种引用，比如引用某些ConfigMap或Secret，这时不清楚该配置的内容，所以无法创建其内容。
>
> 在KubeMin-Cli中打算分成多种机制来减轻这些负担：
>
> 1.如果Statefulset在描述存储的片段中，设置了StorageTraitSpec结构体中的Create=true属性，那么证明该属性是一个独立的存储空间，那么工作流会创建一个Task来构建这个PVC。如果用户的NameSpace空间已经存在这个PVC则不做任何操作，如果不存在，则创建。
>
> 默认情况下Create=false 则仅仅只是引用其PVC的名称。不会创建任何相关性的资源。
>
> 2.如果Statefulset只是引用ConfigMap或Secret的名称，那么不创建任何资源，因为用户可能通过上层的可视化界面来构建
>
> 3.如果Statefulset中的ConfigMap或Secret包含‘data’信息，则会通过创建JobTask来创建相应的资源。
>
> 以上，为了避免资源的互相竞争造成不一致性，工作流会按照一定的执行规则来对相同的ConfigMap或Secret进行编排，后面的会覆盖前面创建的资源。PVC因为不能更改其资源性，后面的编排会无效化。

